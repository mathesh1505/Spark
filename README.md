# Hadoop Introduction: 
Hadoop is an open-source framework designed to store and process large volumes of data (Big Data) efficiently across a cluster of computers.It follows a distributed computing model, meaning work is divided among many machines, enabling faster and scalable processing.
* HDFS (Storage Layer)
* YARN (Resource Management Layer)
* MapReduce (Processing Layer)
# Spark Introduction:
Apache Spark is an open-source, distributed data processing engine designed for fast computation of large-scale data.It supports in-memory processing, making it significantly faster than Hadoop MapReduce.
# Spark Architecture Overview:
